# linear-algebra-for-machine-learning

## week 1 Key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%201/week%201%20lecture%20notes.md))
* dicriminative vs. generative AI
* separation line, regression line, line of best fit, and perceptron
* perceptron line
* linearly separable vs. linear relationship


## week 2 Key take aways
* A matrix is a 2D array with multiple rows and columns
* A vector is a 1D array. It can be _**thought of**_ as a matrix with only one row or one column. But there is no concept of rows and columns in this shape.
* A scalar is just a single number â€” no rows, no columns. A scaler can be used to scale a vector or matrix (e.g., multiply every element by 2)

Dot Product
* The dot product is similar to matrix multiplication but not the same thing since the dot product is when two vectors are being multiplied. It is important to note that vectors are 1D arrays, but can also be 2D arrays if they are a column vector.

The Determinant


