# linear-algebra-for-machine-learning

## week 1 Key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%201/week%201%20lecture%20notes.md))
* dicriminative vs. generative AI
* separation line, regression line, line of best fit, and perceptron
* perceptron line
* linearly separable vs. linear relationship


## week 2 Key take aways
* A matrix is a 2D array with multiple rows and columns
* A vector is a 1D array. It can be _**thought of**_ as a matrix with only one row or one column. But there is no concept of rows and columns in this shape.
* A scalar is just a single number â€” no rows, no columns. A scaler can be used to scale a vector or matrix (e.g., multiply every element by 2)

Dot Product
* The dot product is similar to matrix multiplication but not the same thing since the dot product is when two vectors are being multiplied. It is important to note that vectors are 1D arrays, but can also be 2D arrays if they are a column vector.

The Determinant
* When the derterminant of a matrix is zero that means the matrix does not have a inverse
* When the determinant of the coefficient matrix is zero, it means that the system of linear equations does not have a unique solution, because the equations are not linearly independent. In this case, the system of linear equations could have:
    * Infinitely many solutions (if the system is consistent), or
    * No solution at all (if the system is inconsistent)


