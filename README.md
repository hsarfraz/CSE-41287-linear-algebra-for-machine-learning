# linear-algebra-for-machine-learning

## week 1 Key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%201/week%201%20lecture%20notes.md))
* dicriminative vs. generative AI
* separation line, regression line, line of best fit, and perceptron
* perceptron line
* linearly separable vs. linear relationship


## week 2 Key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%202/week%202%20lecture%20notes.md))
* Defining a matrix (2D array), vector (1D or 2D array), and scalar (single number)
* Dot Product
* The Determinant ([Illustration of deteminant](https://www.youtube.com/watch?v=Ip3X9LOh2dk&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=8))

## week 3 Key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%203/notes.md))
* Gauss Jordan Elimination
* Using elementary row operations (ERO) in Gauss Jordan Elimination
* Augmented matrix
* Pivots
* Under determined Matrix System of Linear Equations
* Over determined Matrix System of Linear Equations
* Dependent Variables
* Free Variables
* Homogeneous system of linear equations
* Non-homogeneous system of linear equations
* Rank of a matrix after Gauss Jordan Elimination
* Trivial solutions of a system of linear equations
* Non-trivial solutions of a system of linear equations
* Moore Penrose Pseudo Inverse

## week 4 Key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%204/notes.md))
* Pivot columns
* Subspace
* Span
* Additvity & Homogeneity
* Dilation (scaling)
* L2 Norm (aka the Euclidean norm)
* Null Space
* Contradiction or no contradictions in a matrix in reduced row echelon form (RREF)
* Orthogonal vectors
* Basis of a vector space

## week 5 key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/blob/main/week%205/notes.md))
* scalars
* eigenvectors
* eigenvalues
* matrix transformations (matrix stretch/compress the eigenvectors by different amounts)
* system of linear equations vs. eigenvectors
* ordered basis
* diagonalizable square matrix
* orthogonal vectors
* orthonormal set of vectors

## week 6 key take aways ([link](https://github.com/hsarfraz/linear-algebra-for-machine-learning/tree/main/week%206))
* SVD helps reduces the dimensions of data that has many dimensions
* Fourier transform (FFT)
* Principle Component Analysis (PCA)
* Correlation
* symmetric matrix
* eigenvalues
* eigenvectors
* diagonaalizable
* singular values and their relation with eigenvalues

## week 7 key take aways ([link]())

## week 8 key take aways ([link]())

## week 9 key take aways ([link]())

## week 10 key take aways ([link]())
